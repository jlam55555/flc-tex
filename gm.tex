\section{G-Machine (GM) implementation}
\label{sec:gm}

The G-Machine is an abstract machine with a very similar idea to the TI machine with graph reduction and unwinding. The difference is that the graph creation, which is a complicated process performed by the instantiation step of the TI machine, is compiled into a series of opcodes on a stack-based machine. The unwinding step remains largely the same. The state of the machine is largely the same, with a stack, dump, and heap. Notably, this also includes an instruction queue (which replaces instantiation), and supercombinators are compiled to a sequence of instructions.

This stack-based machine is desirable for several efficiency reasons: it requires a simpler (smaller) runtime; it does not require the re-traversal of a supercombinator's body expression on each expression; and the runtime does not require an expression-level representation of the language.

\subsection{Sample compilation}
\label{sec:gm-walkthrough}

We first try to develop an intuition of the compilation process. The example below is given in section 3.1.1 of the tutorial.

\begin{minted}{text}
f g x = K (g x)
\end{minted}

This compiles down to the following opcode sequence.

\begin{verbatim}
Push 1
Push 1
Mkap
Pushglobal K
Mkap
Update 3
Pop 3
Unwind
\end{verbatim}

These instructions should be indicative of a stack-based machine. We can envision the state of the program after each instruction. Assume that the supercombinator was invoked like so: \texttt{f I 2}. Thus we expected \texttt{I} and \texttt{2} to be already pushed onto the stack.

Note that this uses the updated addressing mode described in \Cref{sec:gm-addressing-mode}. Thus the stack does not initially contain the application spine, but rather only the root of the supercombinator application and the arguments, pushed in reverse order.

We omit the dump from this evaluation trace; it is not relevant here, since there are no primitive operators.

\begin{verbatim}
-- State 0: initial program state (upon applying f I 2)

Instruction queue: Push 1; Push 1; Mkap; Pushglobal K; Mkap; Update 3; Pop 3; Unwind

Stack:
0. #0
1. #2
2. #4

Heap:
#0 NAp #1 #2
#1 NAp #3 #4
#2 NNum 2
#3 NSupercomb f
#4 NSupercomb I
#5 NSupercomb K
#6 NSupercomb main
\end{verbatim}

Node 0 represents the root of the supercombinator application, which is represented by the following graph:

\begin{verbatim}
Graph(s):
         #0(@)
        /     \
     #3(@)    #2(2)
    /   \
#3(f)   #4(I)
\end{verbatim}

We will now walk through the instruction sequence. The effect of each instruction should be relatively straightforward, since they are relatively simple stack instructions.

\begin{verbatim}
-- State 1: after (first) Push 1
Instruction queue: Push 1; Mkap; Pushglobal K; Mkap; Update 3; Pop 3; Unwind

Stack:
0. #0
1. #2
2. #4
3. #2

Heap:
#0 NAp #1 #2
#1 NAp #3 #4
#2 NNum 2
#3 NSupercomb f
#4 NSupercomb I
#5 NSupercomb K
#6 NSupercomb main

Graph(s):
         #0(@)
        /     \
     #3(@)    #2(2)
    /   \
#3(f)   #4(I)

-- State 2: after (second) Push 1
Instruction queue: Mkap; Pushglobal K; Mkap; Update 3; Pop 3; Unwind

Stack:
0. #0
1. #2
2. #4
3. #2
4. #4

Heap:
#0 NAp #1 #2
#1 NAp #3 #4
#2 NNum 2
#3 NSupercomb f
#4 NSupercomb I
#5 NSupercomb K
#6 NSupercomb main

Graph(s):
         #0(@)
        /     \
     #3(@)    #2(2)
    /   \
#3(f)   #4(I)

-- State 3: after (first) Mkap
Instruction queue: Pushglobal K; Mkap; Update 3; Pop 3; Unwind

Stack:
0. #0
1. #2
2. #4
3. #7

Heap:
#0 NAp #1 #2
#1 NAp #3 #4
#2 NNum 2
#3 NSupercomb f
#4 NSupercomb I
#5 NSupercomb K
#6 NSupercomb main
#7 NAp #4 #2

Graph(s):
         #0(@)
        /     \
     #3(@)    #2(2)
    /   \
#3(f)   #4(I)

   #7(@)
   /   \
#4(I)  #2(2)

-- State 4: after Pushglobal K
Instruction queue: Mkap; Update 3; Pop 3; Unwind

Stack:
0. #0
1. #2
2. #4
3. #7
4. #5

Heap:
#0 NAp #1 #2
#1 NAp #3 #4
#2 NNum 2
#3 NSupercomb f
#4 NSupercomb I
#5 NSupercomb K
#6 NSupercomb main
#7 NAp #4 #2

Graph(s):
         #0(@)
        /     \
     #3(@)    #2(2)
    /   \
#3(f)   #4(I)

   #7(@)
   /   \
#4(I)  #2(2)

-- State 5: after (second) Mkap

Instruction queue: Update 3; Pop 3; Unwind

Stack:
0. #0
1. #2
2. #4
3. #8

Heap:
#0 NAp #1 #2
#1 NAp #3 #4
#2 NNum 2
#3 NSupercomb f
#4 NSupercomb I
#5 NSupercomb K
#6 NSupercomb main
#7 NAp #4 #2
#8 NAp #5 #7

Graph(s):
         #0(@)
        /     \
     #3(@)    #2(2)
    /   \
#3(f)   #4(I)

   #8(@)
   /   \
#5(K)  #7(@)
       /   \
    #4(I)  #2(2)

-- State 6: after Update 3

Instruction queue: Pop 3; Unwind

Stack:
0. #8
1. #2
2. #4
3. #8

Heap:
#0 NAp #1 #2
#1 NAp #3 #4
#2 NNum 2
#3 NSupercomb f
#4 NSupercomb I
#5 NSupercomb K
#6 NSupercomb main
#7 NAp #4 #2
#8 NAp #5 #7

Graph(s):
         #0(@)
        /     \
     #3(@)    #2(2)
    /   \
#3(f)   #4(I)

   #8(@)
   /   \
#5(K)  #7(@)
       /   \
    #4(I)  #2(2)

-- State 7: after Pop 3
Instruction queue: Unwind

Stack:
0. #8

Heap:
#0 NAp #1 #2
#1 NAp #3 #4
#2 NNum 2
#3 NSupercomb f
#4 NSupercomb I
#5 NSupercomb K
#6 NSupercomb main
#7 NAp #4 #2
#8 NAp #5 #7

Graph(s):
   #8(@)
   /   \
#5(K)  #7(@)
       /   \
    #4(I)  #2(2)
\end{verbatim}

As we can see, the graph constructed by the compiled opcodes produces the result as instantiation.

The last instruction is the most complicated. This is the \texttt{Unwind} opcode, which corresponds to the unwinding action discussed in the TI evaluator (setting up the instructions for the next instantiation). Up until now, all of the instructions have been used to create the graph that would have been created through instantiation. After the \texttt{Unwind} opcode, we expect to have a setup similar to state 0 for evaluation to continue again.

\subsection{List of opcodes}
\label{sec:gm-opcodes}

The list of opcodes of the abstract stack machine that the G-Machine compiles to is given below. For the sake of this document, the dynamic semantics of each opcode is only provided informally\footnote{The syntax of this assembly-like language is trivial and need not be stated.}. A precise definition of most of the opcodes are given in the tutorial\footnote{Most of the time, the state transition(s) of an opcode are provided, and the implementation of the opcode in Haskell is left as an exercise.}.

\begin{description}
\item[\texttt{Pushglobal} $f$] Push the address of the global (supercombinator) $f$ onto the stack\footnote{This is overloaded in Mark 6 for a technicality regarding unsaturated data constructors. This is Exercise 3.38.}.
\item[\texttt{Pushint} $n$] Push the address of an integer node representing the integer $n$ onto the stack. If such an integer node representing the integer $n$ already exists, this may use that existing address; otherwise, this will allocate a new integer node $n$.
\item[\texttt{Push} $n$] Push the $n$-th address of the stack onto the stack.
\item[\texttt{Update} $n$] Set the node pointed to by the $n$-th address of the stack to an indirection node pointing to the top element of the stack, then pop one element from the stack.
\item[\texttt{Pop} $n$] Pop $n$ elements from the stack.
\item[\texttt{Alloc} $n$] Push $n$ invalid addresses onto the stack, or decrease the stack pointer by $n$ \footnote{This is used for \texttt{letrec} expressions. These addresses initially point to an invalid node, and will be updated by the evaluation of the definiens.}.
\item[\texttt{Slide} $n$] ``Slide'' the top element of the stack up $n$ slots. In other words, the $(n+1)$-th element of the stack will be replaced with the top of the stack, and then $n$ elements will be popped.
\item[\texttt{Unwind}] Similar in function to unwinding in the TI implementation \Cref{sec:ti-unwind}. Finds the next redex along the spine. Also performs the stack restructuring described in \Cref{sec:gm-addressing-mode}. When the supercombinator or primitive is found, adds the compiled opcodes to the instruction queue.
\item[\texttt{Mkap}] Pop two elements from the stack, allocate a new application node (with the first popped element in function position, and the second popped element in argument position) and push the application node onto the stack.
\item[\texttt{Eval}] Strictly evaluate the element on the top of the stack to WHNF on a new stack (frame)\footnote{This pushes a new stack on the dump and evaluates the node on the new stack. This is more or less equivalent to a function call in the conventional procedural ABIs.}.
\item[\texttt{Add}, \texttt{Sub}, \texttt{Mul}, \texttt{Div}, \texttt{Neg}] Arithmetic binary and unary operators. Assumes the two elements on the top of the stack are evaluated to integers (otherwise will crash the program). Pops the top two elements and leaves a number node on the top of the stack representing the arithmetic result.
\item[\texttt{Eq}, \texttt{Ne}, \texttt{Lt}, \texttt{Le}, \texttt{Gt}, \texttt{Ge}] Relational binary operators. Assumes the two elements on the top of the stack are evaluated to booleans in the standard representation (otherwise will crash the program). Pops the top two elements and leaves a node on the top of the stack representing the boolean result in the standard representation.
\item[\texttt{Pack} $t$ $n$] Pops $n$ addresses from the stack. Allocate and push a structured data node representing structured data with tag $t$ and the $n$ popped arguments.
\item[\texttt{Casejump} $rules$] The set $rules$ is of the form $\{t_j\to [i_1,i_2,\dots]\}$: a mapping of tags $t_j$ to code sequences $[i_1,i_2,\dots]$. Assumes the top of the stack is the scrutinee of a \texttt{case} expression, and thus a data node with tag $t$. Appends the instruction list of the appropriate rule onto the current instruction queue. Throws an error if the scrutinee is not a structured data node or if there is no match.
\item[\texttt{Split} $n$] Assumes the top of the stack is a structured data node. Pops it from the stack, and then pushes its $n$ arguments onto the stack.
\item[\texttt{Print}] Output the top element on the stack. Assumes that it is a data node (integer or structured data node). Recursively generates additional \texttt{Print} and \texttt{Eval} opcodes for structured data.

\end{description}

\subsection{A simpler stack addressing mode}
\label{sec:gm-addressing-mode}

In the template instantiation, the spine was pushed onto the stack. This means that for a supercombinator with $n$ arguments, the spine has $n$ \texttt{NAp} nodes, followed by the supercombinator node at the top of the stack. In order to get the $m$-th argument, we traverse up $(m+1)$ nodes, and have to perform an extra lookup on the \texttt{NAp} to get the address of the argument.

Mark 3 of the G-machine proposes a simpler stack addressing scheme. To prevent extra indirections, we restructure the stack during an \texttt{Unwind} instruction so that only the arguments for a supercombinator are left on the stack, rather than the \texttt{NAp} nodes. The exception is that the root of the supercombinator application redex is left on the stack, so that it may be updated with the evaluation result of the redex. This method extends naturally to \texttt{let(rec)} expressions.

This aligns with the x86 ABI, which involves pushing arguments onto the stack in reverse order. Of course, there is no notion of application nodes in a low-level language. The only difference is the root of the redex remaining under the arguments on the stack.

\subsection{Compilation schemes}
\label{sec:compilation-schemes}

\todo{list each compilation scheme, describe it, show its behavior on expression types}

\todo{n.b. i have no idea how these evaluation schemes are named}

\subsubsection{Non-strict vs. strict compilation context}
\label{sec:strict-context}

Mark 4 of the G-machine introduces primitives into the language, and is the first iteration of the G-machine in which strictness becomes relevant. The ordinary compilation of an primitive operation such as addition produces the following opcode sequence.

\begin{verbatim}
Push 1
Eval
Push 1
Eval
Add
Update 2
Pop 2
Unwind
\end{verbatim}

However, this seems like an awful lot of work to perform a simple addition operation. The book motivates this with the simple program:

\begin{minted}{text}
main = 3+4*5
\end{minted}

Ordinarily, this program will compile to the following program. In addition to this opcode sequence, each of the function applications will spawn the eight opcodes shown above.

\begin{verbatim}
Pushint 5
Pushint 4
Pushglobal "*"
Mkap
Mkap
Pushint 3
Pushglobal "+"
Mkap
Mkap
Eval
\end{verbatim}

However, we can compile this to the following set of opcodes by inspection, which is much shorter, and doesn't include any function applications or \texttt{Eval} opcodes:

\begin{verbatim}
Pushint 5
Pushint 4
Mul
Pushint 3
Add
\end{verbatim}

It should not be surprising that strict code may be more efficient than non-strict code. This gives rise to the $\mathcal{E}$ evaluation scheme described in \Cref{sec:compilation-schemes}. However, we cannot always generate strict code, or else defeat the purpose of a lazy language. In particular, we cannot generate strict code for function arguments or definiens in a \texttt{let(rec)} expression, since these may never be evaluated and evaluating them strictly would make evaluation not normal-order.

The intuition behind the $\mathcal{E}$ compilation scheme is that any expression that lies directly in a supercombinator body (i.e., not in a \texttt{let(rec) definiens or in a function argument}) must be evaluated when the supercombinator is evoked. We compiled such expressions in the \texttt{strict evaluation context} $\mathcal{E}$. Other expressions are compiled in the \texttt{lazy evaluation context} $\mathcal{C}$. Additionally, we do not need \texttt{Eval} opcodes when in the strict compilation context, since we can recursively deduce that all subexpressions are strictly evaluated.

Strict and lazy evaluation is purely for improvements in performance, and do not (should not) change the behavior of the program. The current mechanism to determine when a strict evaluation context is acceptable is very simple; the tutorial cites other research on the material such as \cite{burn1991lazy}. Haskell also allows users to explicitly specify strict evaluation of an expression.

\subsection{Evaluator}
\label{sec:evaluator}

The evaluator implements the abstract stack machine. Each of the opcodes described in \Cref{sec:gm-opcodes} is implemented as a separate function in the G-Machines's \texttt{Evaluator} module.

\subsubsection{Transpilation to x86-64 assembly code}
\label{sec:gm-transpile-x86}

To illustrate the simplicity of the G-Machine's runtime, the stack machine of the Mark 1 of the G-Machine was implemented in NASM x86-64 assembly code. Each stack machine opcode was implemented using an assembly macro. Supercombinator node definitions are also implemented with a macro. This can be found on GitHub at \href{https://github.com/jlam55555/flc-transpiler}{jlam55555/flc-transpiler}.

I did not have time to implement later Marks of the G-Machine in assembly code, and I believe that it is a reasonable project for future work. However, the increased complexity of later Marks will likely be convenienced by small helper functions written in C.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
